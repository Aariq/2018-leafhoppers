---
title: "2017 GC Data Wrangling and outlier removal"
author: Eric R. Scott
date: "2018-04-30"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
    highlight: kate
    theme: yeti
---
```{r setup}
knitr::opts_chunk$set(
	echo = TRUE
)
```
*Last compiled: `r Sys.Date()`*

# Packages

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse) #for data manipulation
library(here)
library(chemhelper)
library(webchem)

#for outlier detection:
library(MVN)
library(HDoutliers)
library(ropls)

#resolve conflicts
map <- purrr::map 
mvn <- MVN::mvn
```


# Workflow Overview

1. Tag 0's as nondetects
2. Tag 0<Area<1000 as censored
3. Filter out known contaminants
4. Calculate RPA
5. Split data into samples and method blank
6. Subtract RPAs of method blank

This will give you sample data with 0s and possibly negative values for RPA and flags for nondetects and peaks that should be censored.

7. Remove compounds that are not found in more than 2 samples (at least 3 samples) with RPA > 0
8. Set RPA for nondetects, censored, and all RPA <= 0  to 500/IS
9. Check for and remove any obvious outliers
10. Create wide dataframe

# Read in files

```{r directories}
#this is the directory that contains data files loaded into the project already
Data.dir <- "/Volumes/as_rsch_orianslab_tea01$/IonAnalytics/IonAnalytics Projects/Fujian 2017 Manipulative/Data"

if(!dir.exists(Data.dir)) {
  stop("Gotta connect to the network drive!")
}

files <- dir(Data.dir,
             pattern = "Fujian Manipulative Adjusted2 Integration Report.csv",
             full.names = TRUE, #adds full path
             recursive = TRUE) #drills down in directory structure

filenames <- str_extract(files, "(?<=/)[^/]+(?=/[^/]+$)")
# samples <- str_extract(filenames, "(?<=Hop_).+(?=\\.D)")
gc_tidy <- files %>%
  map(read_IA) %>%
  set_names(filenames) %>% 
  bind_rows(.id = "filename") %>% 
  rename(area = `Area [a.u.*s]`,
         rt = `R.Time [min.]`)
```

## Alkane Files

```{r}
alkanes <-
  dir(Data.dir,
    pattern = "Alkane",
    full.names = TRUE,
    recursive = TRUE) %>% 
  read_IA() %>%
  mutate(rt = ifelse(`R.Time [min.]` == 0, `Expect. [min.]`, `R.Time [min.]`)) %>% 
  select(Compound, rt) %>% 
  add_column(C_num = 6:21)
alkanes
```

## Method file with CAS numbers

```{r}
method <- read_IA("/Volumes/as_rsch_orianslab_tea01$/IonAnalytics/IonAnalytics Projects/Fujian 2017 Manipulative/Methods/Fujian Manipulative Adjusted 2/Fujian Manipulative Adjusted2 Compound References.csv")
```

Merge with data

```{r}
CASs <-
  method %>%
  select(No., Compound, CAS) %>% 
  mutate(CAS = as.cas(CAS)) %>% 
  #try to catch all the unknowns
  mutate(CAS = ifelse(str_detect(Compound, "^\\d+$") | str_detect(Compound, "DCSE"), NA, CAS))
gc_tidy <-
  left_join(gc_tidy, CASs) %>% select(filename, No., Type, Compound, CAS, everything())
```

## Extract sample name from filename, set cultivar

```{r}
gc_tidy <- 
  gc_tidy %>% 
  mutate(cultivar = "Q",
         plant_num = str_extract(filename, "(?<=Hop_)\\d+(?=_)"), #blanks will be NA
         sample = ifelse(!is.na(plant_num), paste0(cultivar, plant_num), filename)) %>% 
  select(filename, cultivar, plant_num, sample, everything())
```



# Tag non-detects and to-be-censored

```{r}
gc_tidy <-
  gc_tidy %>%
  mutate(ND = area == 0,
         Censor = area > 0 & area < 1000)
```

# Remove known contaminants and problematic peaks
Contaminants:

- Propanoic acid, 2-methyl-, 1-(1, 1-dimethylethyl)-2methyl-1, 3-propanediyl ester (from nitrile gloves)
- Toluene (very large peak, seems unlikely that it is a natural product)

These chlorinated compounds are likely pesticides:

- Benzaldehyde, 2,6-dichloro-
- Benzene, chloro-

These compounds should be removed for other reasons:

- 141: Unknown that shows up in method blanks.  Nicole doesn't know what it is, but is pretty sure it is contaminant.
- Anything with an RT earlier than 3.3-ish (currently corresponds to No. > 91)

Problematic peaks includes peaks that I decided were bad, and should be marked as "not found" in all files, but was too lazy to go through and mark in Ion Analytics.  It's much easier to just exclude them in R.

 - (135) Heptane, 2,4-dimethyl-: Check all.  Bad peak, borderline Q value.
 - (181) 1 (DCSE): Hitting same peak as (180) Pentane, 1-nitro-.  Mark all as not found.  Remove from method when done with deconvolution so it doesn't get propogated to the next dataset.
 - (234) 37: Pretty bad peak.  Mark all as not found?
 - (268, 269): unknown 64 = limonene? Mark 64 as not found.
 - (329) 2-Nonanone: Bad peak.  Mark all as not found
 - (345) Fenchol<endo->: Bad peak.  Mark all as not found
 - (408) 158: Matches a few other peaks.  Mark all as not found
 - (419) 127: one ion low.  mark all as not found. Consider removing from method (could be decanal + noise?)
 - (554) Geranylacetone: Hits same peak as (556) 5,9-Undecadien-2-one...  Mark all as not found.

```{r}
contaminants <- c("Propanoic acid, 2-methyl-, 1-(1,1-dimethylethyl)-2-methyl-1,3-propanediyl ester",
                  "Toluene",
                  "Benzyl chloride",
                  "Benzene, chloro-",
                  "Benzaldehyde, 2,6-dichloro-",
                  "141")

problematic <- c(135, 181, 234, 268, 329, 345, 408, 419, 554)
gc_tidy.1 <- gc_tidy %>%
  filter(!Compound %in% contaminants) %>%
  filter(No. > 91) %>% 
  filter(!No. %in% problematic)
```


# Calculate RPA

```{r}
#extract napthalene D8 area
gc_IS <- gc_tidy.1 %>% 
  filter(Compound == "Naphthalene-D8") %>%
  rename(IS = area) %>% 
  select(sample, IS)

#join with gc.tidy and mutate to calculate RPA
gc_tidy.2 <- full_join(gc_tidy.1, gc_IS)
gc_tidy.2 <- gc_tidy.2 %>% 
  mutate(RPA = area/IS) %>% 
  #don't need Naphtalene-D8 anymore
  filter(Compound != "Naphthalene-D8")
gc_tidy.2
```

# Field blank subtraction

I have two field blanks, one from the beginning and one from the end.  I could average them and subtract from all samples or subtract the start blank from the start samples and the end blank from the end samples.

For now, I'm just using the end data, so I'll just use the end blank.

```{r}
#split into samples and background
gc_blank <-
  gc_tidy.2 %>%
  filter(str_detect(filename,"blank_end"))
gc_tidy.3 <-
  gc_tidy.2 %>% 
  filter(!str_detect(filename, "blank"))

#subtract background
gc_blank.1 <-
  gc_blank %>%
  rename(background = RPA) %>% 
  select(Compound, background)

gc_tidy.3 <- left_join(gc_tidy.3, gc_blank.1)

gc_tidy.4 <-
  gc_tidy.3 %>%
  mutate(RPA = RPA - background)
```




# Read in treatment data and join

```{r}
treatments <- read_rds(here('data', 'cleaned', '2017_treatment_data.rds'))
```


```{r}
gc_tidy.5 <- left_join(gc_tidy.4, treatments)
```

# Calculate retention indices

```{r}
gc_tidy.6 <-
  gc_tidy.5 %>% 
  mutate(ri = calc_RI(rt, alkanes$rt, alkanes$C_num))
```

# Add CAS numbers
TODO

# Deal with non-detects, censored peaks, and other zeros and negative values

Nondetects were zeros to begin with, not found by IonAnalytics at all.  Censored peaks are those with peak area < 1000.  According to the Robbat lab, areas < 1000 are not accurate due to being below the LOD.  There are multiple options for dealing with this, but they usually set those peaks to zero.  I'll set them all to a small number (500/IS).  This will be useful for log-transforming data since log(0) = -inf.

```{r}
gc_tidy.7 <-
  gc_tidy.6 %>%                             # Set RPA to a very small number...
  mutate(RPA = case_when(ND            ~ 500/IS, # if peaks were always not detected
                         Censor        ~ 500/IS, # or if their original areas were  < 1000
                         RPA < 500/IS  ~ 500/IS, # or if their areas after background subtraction < 500
                         TRUE    ~ RPA),
         present = case_when(ND           ~ FALSE,
                             Censor       ~ FALSE,
                             RPA <= 500/IS ~ FALSE,
                             TRUE         ~ TRUE)) %>%
  select(sample, cultivar, density_start, density_end, mean_percent_damage, twister_damage, No., Compound, CAS, RPA, rt, ri, present)

#How many compounds detected in each sample?
gc_tidy.7 %>%
  group_by(sample) %>%
  summarize(num_compounds = sum(present))
```

Preserve zeroes for distance-based RDA:

```{r}
gc_zeroes <-
  gc_tidy.7 %>% 
  mutate(RPA = ifelse(present, RPA, 0))
```

# Remove rare peaks

Remove compounds that are found in **5 or fewer** samples.

```{r}
gc_tidy.final <-
  gc_tidy.7 %>% 
  group_by(Compound) %>% #for each compound...
  filter(sum(present) >= 5)
gc_tidy.final %>% ungroup() %>% count(sample) #down to 94 compounds

gc_zeroes.final <-
  gc_zeroes %>% 
  group_by(Compound) %>% 
  filter(sum(present) >= 5)
gc_zeroes.final %>% ungroup() %>% count(sample) #just checking that it's the same
```
0.001717365

# Create wide data frame

```{r}
gc_wide <- 
  gc_tidy.final %>%
  ungroup() %>% 
  select(-rt, -ri, -No., -present, -CAS) %>%
  spread(key = Compound, value = RPA)

gc_wide.zeroes <-
  gc_zeroes.final %>% 
  ungroup %>% 
  select(-rt, -ri, -No., -present, -CAS) %>% 
  spread(key = Compound, value = RPA)
```


# Outlier detection

## Multivariate normality

For some reason (data too large? numbers too small?) the `mvn()` function often fails due to a singular matrix error.  Below, I've run it on random subsets of the data and log transformation seems to improve normality. Interestingly, scaling seems to make things worse.

```{r}
gc_tidy.final %>% 
  ggplot(aes(RPA)) + geom_histogram()
gc_tidy.final %>% 
  ggplot(aes(log(RPA))) + geom_histogram()
```
 
```{r}
metavars <- c("sample", "cultivar", "density_start", "density_end", "mean_percent_damage", "twister_damage")

test <-
  gc_tidy.final %>%
  ungroup() %>% 
  select(-rt, -ri, -No., -present, -CAS) %>% 
  spread(key = Compound, value = RPA) %>% 
  select(-metavars) %>%
  # mutate_all(~.*1000) %>%
  mvn(mvnTest = "mardia",
      transform = "log",
      multivariatePlot = "qq",
      scale = TRUE)
test
```

So this dataset is clearly zero-inflated (or rather, small number introduced by data pre-treatment inflated).  I should consider distance-based methods as a sanity-check.

## Multivariate outlier detection

Using Leland Wilkinson's algorithm from `HDoutliers`

```{r}
library(HDoutliers)
out1 <- HDoutliers(select(gc_wide, -metavars), alpha = 0.1)
gc_wide[out1, ]$sample
#after log transformation
out2 <- HDoutliers(select(gc_wide, -metavars) %>% mutate_all(log), alpha = 0.1)
gc_wide[out2, ]
```

No outliers detected


## With PCA

```{r}
library(ropls)
pca.1 <- opls(select(gc_wide, -metavars), scaleC = "standard", plotL = FALSE)
pca.2 <-
  gc_wide %>%
  select(-metavars) %>%
  mutate_all(log) %>%
  opls(scaleC = "standard", plotL = FALSE)

plot(pca.2, parLabVc = gc_wide$sample)#log transformed
# plot(pca.1, parLabVc = gc_wide$sample) 
```

Q8 and Q13 shows up as an outlier in untransformed data.

I'm not going to remove this outlier.  I'm not convinved it's important to remove.


# Write to file

```{r}
write_rds(gc_wide, here("data", "cleaned", "2017_gcms_wide.rds"))
write_rds(gc_tidy.final, here("data", "cleaned", "2017_gcms_tidy.rds"))
write_rds(gc_wide.zeroes, here("data", "cleaned", "2017_gcms_zeroes.rds"))
```